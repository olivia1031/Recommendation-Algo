{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math, os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'\n",
    "          .format(end_mem,100*(start_mem-end_mem)/start_mem,(time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻推荐\n",
    "\n",
    "数据来自某新闻APP平台的用户交互数据：\n",
    "- 包括<mark>30万用户，近300万次点击log\n",
    "- 共36万多篇不同的新闻文章，同时<mark>每篇新闻文章有对应的embedding向量表示。\n",
    "\n",
    "为了保证比赛的公平性，将会从中\n",
    "- 抽取20万用户的点击日志数据作为<mark>训练集，\n",
    "- 5万用户的点击日志数据作为<mark>测试集A，\n",
    "- 5万用户的点击日志数据作为<mark>测试集B。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "save_path = './results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug模式：从训练集中划出一部分数据来调试代码\n",
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    \"\"\"\n",
    "        训练集中采样一部分数据调试\n",
    "        data_path: 原数据的存储路径\n",
    "        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique() #train_click_log中所有去重的user_id\n",
    "\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False) #选一部分user_id作为训练集\n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click\n",
    "\n",
    "# 读取点击数据，这里分成线上和线下，如果是为了获取线上提交结果应该讲测试集中的点击数据合并到总的数据中\n",
    "# 如果是为了线下验证模型的有效性或者特征的有效性，可以只使用训练集\n",
    "def get_all_click_df(data_path='./data/', offline=True):\n",
    "    if offline:\n",
    "        all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    else:\n",
    "        trn_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "        all_click = trn_click.append(tst_click)\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量训练集\n",
    "all_click_df = get_all_click_df(offline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题介绍\n",
    "\n",
    "正确了解赛题背后的思想以及赛题业务逻辑的清晰，有利于花费更少时间构建更为有效的特征模型：\n",
    "- 目的：<mark>根据用户历史浏览点击新闻文章的数据信息，预测用户的最后一次点击的新闻文章<mark>（用户未来的点击行为）\n",
    "\n",
    "这道赛题的设计初衷是引导大家了解推荐系统中的一些业务背景， 解决实际问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取采样或全量数据\n",
    "\n",
    "#### train_click_log（都是数值）:\n",
    "\n",
    "| user_id| click_article_id | click_timestamp| click_environment|click_deviceGroup|\n",
    "| -- | --  | -- | -|--|\n",
    "| user_id| 点击文章id | 点击时间戳| 点击环境|点击设备组|\n",
    "\n",
    "|click_os|click_country|click_region|click_referrer_type|\n",
    "|--|--|--|--|\n",
    "|点击操作系统|国家|地区|点击来源类型|\n",
    "\n",
    "\n",
    "#### article\n",
    "| article_id|category_id|created_at_ts|words_count| emb_1,emb_2,…,emb_249 |\n",
    "| --|--|--|--| --|\n",
    "| 文章id|文章类别id|文章创建时间戳|文章字数|文章embedding向量表示|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价方式理解\n",
    "\n",
    "理解评价方式， 我们需要结合着最后的提交文件来看， 根据sample.submit.csv， 我们最后提交的格式是针对每个用户， 我们都会给出五篇文章的推荐结果，按照点击概率从前往后排序。 \n",
    "\n",
    "而真实的每个用户最后一次点击的文章只会有一篇的真实答案， 所以我们就看我们推荐的这五篇里面是否有命中真实答案的。\n",
    "\n",
    "比如对于user1来说， 我们的提交会是：\n",
    "\n",
    "    user1, article1, article2, article3, article4, article5.\n",
    "\n",
    "<mark>评价指标的公式如下：\n",
    "    score(user)=[5∑k=1] s(user,k)/k\n",
    "\n",
    "\n",
    "\n",
    "- 假如article1就是真实的用户点击文章，也就是article1命中， 则s(user1,1)=1, s(user1,2-4)都是0， \n",
    "- 如果article2是用户点击的文章， 则s(user,2)=1/2,s(user,1,3,4,5)都是0。\n",
    "- 也就是score(user)=命中第几条的倒数。\n",
    "- 如果都没中， 则score(user1)=0。 \n",
    "\n",
    "这个是合理的， 因为我们希望的就是命中的结果尽量靠前， 而此时分数正好比较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题理解\n",
    "\n",
    "与普通数据挖掘的区别：\n",
    "- 目标上， 要预测最后一次点击的新闻文章，也就是我们给用户推荐的是新闻文章， 并不是普通的回归或分类；\n",
    "- 数据上， 也不是我们之前遇到的那种特征+标签的数据，而是基于了真实的业务场景， 拿到的用户的点击日志；\n",
    "\n",
    "那么，如何把该预测问题转成一个监督学习的问题(特征+标签)，然后进行ML，DL等建模预测？\n",
    "- 如何转成一个监督学习问题呢？ \n",
    "- 转成一个什么样的监督学习问题呢？ \n",
    "- 我们能利用的特征又有哪些呢？ \n",
    "- 又有哪些模型可以尝试呢？ \n",
    "- 此次面对数万级别的文章推荐，我们又有哪些策略呢？\n",
    "\n",
    "=> 从36万篇文章中预测某一篇的话,可能是一个多分类的问题(36万类里面选1)，但太过庞大；\n",
    "\n",
    "=> <mark>预测出某个用户最后一次对于某一篇文章会进行点击的概率 CTR，概率最大的那篇文章就是用户最后一次可能点击的新闻文章 </mark>; 这样就把原问题变成了一个点击率预测的问题(用户, 文章) --> 点击的概率(软分类),对于模型的选择就基本上有大致方向了，比如最简单的逻辑回归模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类问题 CTR预估\n",
    "\n",
    "大致的解决思路：\n",
    "- 转成一个<mark>分类问题;\n",
    "- 分类的<mark>标签 Y 就是用户是否会点击某篇文章//二分类;\n",
    "- 分类问题的<mark>特征 X 如用户和文章;\n",
    "- 训练一个<mark>分类模型， 对某用户最后一次点击某篇文章的概率进行预测;\n",
    "\n",
    "那么又会有几个问题：\n",
    "- 如何转成监督学习问题？ \n",
    "- 训练集和测试集怎么制作？ \n",
    "- 我们又能利用哪些特征？ \n",
    "- 我们又可以尝试哪些模型？ \n",
    "- 面对36万篇文章， 20多万用户的推荐， 我们又有哪些策略来缩减问题的规模？\n",
    "- 如何进行最后的预测？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取 用户-文章-点击事件 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据点击时间获取用户的点击文章序列   {user1: [(item1, time1), (item2, time2)..]...}\n",
    "def get_user_item_time(click_df):\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    \n",
    "    def make_item_time_pair(df):\n",
    "        return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "    \n",
    "    user_item_time_df = click_df.groupby('user_id')['click_article_id', 'click_timestamp']\\\n",
    "                                                            .apply(lambda x: make_item_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'item_time_list'})\n",
    "    user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "    \n",
    "    return user_item_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '一', 2: '二', 3: '三', 4: '四', 5: '五', 6: '六'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 例子\n",
    "tmp_df = pd.DataFrame({\"class\":[1,2,1,2,1,2],\n",
    "                       \"idx\":[1,2,3,4,5,6],\n",
    "                       \"标号\":[\"一\",\"二\",\"三\",\"四\",\"五\",\"六\"]\n",
    "})\n",
    "dict(zip(tmp_df[\"idx\"],tmp_df[\"标号\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取点击最多的TopK个文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取近期点击最多的文章\n",
    "def get_item_topk_click(click_df, k):\n",
    "    topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "    return topk_click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### itemCF的物品相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': {'j': 0}}  and  0\n"
     ]
    }
   ],
   "source": [
    "tmp_dict = dict()\n",
    "tmp_dict.setdefault(\"i\", {})\n",
    "tmp_dict[\"i\"].setdefault(\"j\", 0)\n",
    "print(tmp_dict,\" and \", tmp_dict[\"i\"][\"j\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df):\n",
    "    \"\"\"\n",
    "        文章与文章之间的相似性矩阵计算\n",
    "        :param df: 数据表\n",
    "        :item_created_time_dict:  文章创建时间的字典\n",
    "        return : 文章与文章的相似性矩阵\n",
    "        思路: 基于物品的协同过滤， 在多路召回部分会加上关联规则的召回策略\n",
    "    \"\"\"\n",
    "    user_item_time_dict = get_user_item_time(df) #{user1: [(item1, time1), (item2, time2)..]...}\n",
    "    \n",
    "    # 计算物品相似度\n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)#{item_i:{item_j:Wij}}\n",
    "    \n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()): #tqdm(iterator)在长循环中添加一个进度提示信息\n",
    "        # 在基于商品的协同过滤优化的时候可以考虑时间因素\n",
    "        for i, i_click_time in item_time_list:\n",
    "            item_cnt[i] += 1 #item总共被点击几次\n",
    "            i2i_sim.setdefault(i, {}) #若不存在，创建一个dict element:{item_i:{}}\n",
    "            for j, j_click_time in item_time_list:\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                i2i_sim[i].setdefault(j, 0) \n",
    "                '''{item_i:{item_j:0}} = item_i和item_j的相似性分子，同时被一个user点击，+1'''\n",
    "                i2i_sim[i][j] += 1 / math.log(len(item_time_list) + 1)\n",
    "                \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items(): #对每个item_pair除以分母（item_i总点击次数，根号，同item_j,相乘）\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb')) \n",
    "    #pickle.dump：将对象obj保存到文件file中去，“write binary”\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 250000/250000 [00:37<00:00, 6623.98it/s]\n"
     ]
    }
   ],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itemCF 的文章推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于商品的召回i2i\n",
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click):\n",
    "    \"\"\"\n",
    "        基于文章协同过滤的召回\n",
    "        :param user_id: 用户id\n",
    "        :param user_item_time_dict: 字典,{user1: [(item1, time1), (item2, time2)..]...}\n",
    "        :param i2i_sim: 字典，文章相似性矩阵, {item_i:{item_j:Wij}}\n",
    "        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章\n",
    "        :param recall_item_num: 整数， 最后的召回文章数量\n",
    "        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全        \n",
    "        return: 召回的文章列表 [item1:score1, item2: score2...]\n",
    "        注意: 基于物品的协同过滤， 在多路召回部分会加上关联规则的召回策略\n",
    "    \"\"\"\n",
    "    \n",
    "    # 获取用户历史交互的文章\n",
    "    user_hist_items = user_item_time_dict[user_id] # 注意，此时获取得到的是一个元组列表，需要将里面的user_id提出来\n",
    "    user_hist_items_ = {item_id for item_id, _ in user_hist_items} #此用户点击过的{item_id, ....}\n",
    "\n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items): #遍历用户点击过的所有item，找每个item的相似文章\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "        #对于每一个点击的文章item_i, 按Wij排序，遍历topK 与i最相似的前k篇文章, 加入到推荐列表\n",
    "            if j  in user_hist_items_:\n",
    "                continue       \n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] +=  wij\n",
    "    \n",
    "    # 不足10个，用热门商品补全\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            item_rank[item] = - i - 100 # 随便给个负数就行，排在推荐列表的最后面\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "    \n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num] #最后的召回文章\n",
    "        \n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给每个用户根据物品的协同过滤推荐文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 250000/250000 [53:46<00:00, 77.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "\n",
    "# 获取 用户 - 文章 - 点击时间的字典 => {user1: [(item1, time1), (item2, time2)..]...}\n",
    "user_item_time_dict = get_user_item_time(all_click_df)\n",
    "\n",
    "# 去取文章相似度 => {item_i:{item_j:Wij, ...}}\n",
    "i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "# 相似文章的数量\n",
    "sim_item_topk = 10\n",
    "\n",
    "# 召回文章数量\n",
    "recall_item_num = 10\n",
    "\n",
    "# 用户热度补全\n",
    "item_topk_click = get_item_topk_click(all_click_df, k=50)\n",
    "\n",
    "for user in tqdm(all_click_df['user_id'].unique()): #遍历每一个有过点击行为的user，推荐 n个文章\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, \n",
    "                                                        sim_item_topk, recall_item_num, item_topk_click)\n",
    "                                   #return item_rank = dict({item_recom: score, ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 召回字典转换成df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 250000/250000 [00:08<00:00, 30787.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# 将字典的形式转换成df\n",
    "user_item_score_list = []\n",
    "\n",
    "for user, items in tqdm(user_recall_items_dict.items()): # {user:{item_recom: score, ...},  ...}\n",
    "    for item, score in items:\n",
    "        user_item_score_list.append([user, item, score])\n",
    "\n",
    "recall_df = pd.DataFrame(user_item_score_list, columns=['user_id', 'click_article_id', 'pred_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成提交文件\n",
    "def submit(recall_df, topk=5, model_name=None):\n",
    "    recall_df = recall_df.sort_values(by=['user_id', 'pred_score'])\n",
    "    recall_df['rank'] = recall_df.groupby(['user_id'])['pred_score'].rank(ascending=False, method='first')\n",
    "    # rank over(partition by 'user_id', order by 'pred_score')\n",
    "    \n",
    "    # 判断是不是每个用户都有5篇文章及以上\n",
    "    tmp = recall_df.groupby('user_id').apply(lambda x: x['rank'].max())\n",
    "    assert tmp.min() >= topk\n",
    "    \n",
    "    #df['user_id', 'click_article_id', 'pred_score','rank']\n",
    "    del recall_df['pred_score']\n",
    "    #将 ['user_id', 'rank'] 作为索引 #unstack把 相同的user_id的数据 摞起来：故一行有5个rank顺序的article_id\n",
    "    submit = recall_df[recall_df['rank'] <= topk].set_index(['user_id', 'rank']).unstack(-1).reset_index()\n",
    "    \n",
    "    submit.columns = [int(col) if isinstance(col, int) else col for col in submit.columns.droplevel(0)]\n",
    "    # 按照提交格式定义列名\n",
    "    submit = submit.rename(columns={'': 'user_id', 1: 'article_1', 2: 'article_2', \n",
    "                                                  3: 'article_3', 4: 'article_4', 5: 'article_5'})\n",
    "    \n",
    "    save_name = save_path + model_name + '_' + datetime.today().strftime('%m-%d') + '.csv'\n",
    "    submit.to_csv(save_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取测试集\n",
    "tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "tst_users = tst_click['user_id'].unique()\n",
    "\n",
    "# 从所有的召回数据中将测试集中的用户选出来\n",
    "tst_recall = recall_df[recall_df['user_id'].isin(tst_users)]\n",
    "\n",
    "# 生成提交文件\n",
    "submit(tst_recall, topk=5, model_name='itemcf_baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
